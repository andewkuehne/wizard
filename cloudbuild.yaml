steps:
# Step 1: Check, delete if exists, and recreate MongoDB VM
- name: 'gcr.io/cloud-builders/gcloud'
  id: 'manage-mongodb-vm'
  entrypoint: 'bash'
  args:
  - '-c'
  - |
    set -e
    if gcloud compute instances describe mongodb-vm --zone=${_ZONE} &>/dev/null; then
      echo "MongoDB VM exists. Deleting..."
      gcloud compute instances delete mongodb-vm --zone=${_ZONE} --quiet
      echo "Waiting for VM deletion..."
      while gcloud compute instances describe mongodb-vm --zone=${_ZONE} &>/dev/null; do
        sleep 5
      done
    else
      echo "MongoDB VM does not exist."
    fi
    echo "Creating MongoDB VM..."
    gcloud compute instances create mongodb-vm \
    --image-family ubuntu-2004-lts \
    --image-project ubuntu-os-cloud \
    --machine-type ${_MACHINE_TYPE} \
    --zone ${_ZONE} \
    --scopes=https://www.googleapis.com/auth/devstorage.read_write

# Capture the service account of the VM and append to env_vars file
gcloud compute instances describe mongodb-vm --zone=${_ZONE} --format='value[separator="="](name,"VM_SERVICE_ACCOUNT="serviceAccounts[0].email)' >> /workspace/env_vars
    
# Step 2: Check, archive, and create Backup Bucket
- name: 'gcr.io/cloud-builders/gsutil'
  id: 'manage-backup-bucket'
  entrypoint: 'bash'
  args:
  - '-c'
  - |
    set -e
    # Load the VM service account
    source /workspace/env_vars
    
    if gsutil ls -b gs://${_BACKUP_BUCKET} &>/dev/null; then
      echo "Backup bucket exists. Archiving..."
      if ! gsutil ls -b gs://${PROJECT_ID}-archive &>/dev/null; then
        gsutil mb -p ${PROJECT_ID} gs://${PROJECT_ID}-archive
      fi
      gsutil -m rsync -r gs://${_BACKUP_BUCKET} gs://${PROJECT_ID}-archive
      gsutil rm -r gs://${_BACKUP_BUCKET}
      echo "Bucket Archived to ${PROJECT_ID}-archive"
    else
      echo "Backup bucket does not exist."
    fi
    echo "Creating new backup bucket..."
    gsutil mb -p ${PROJECT_ID} gs://${_BACKUP_BUCKET}
    echo "New backup bucket created: gs://${_BACKUP_BUCKET}"
    
    echo "Setting permissions for the VM to access the bucket..."
    gsutil iam ch serviceAccount:${VM_SERVICE_ACCOUNT}:objectAdmin gs://${_BACKUP_BUCKET}
    echo "Bucket permissions set for VM access."
    
    echo "Verifying bucket permissions:"
    gsutil iam get gs://${_BACKUP_BUCKET}
# Step 3: Install MongoDB 
- name: 'gcr.io/cloud-builders/gcloud'
  id: 'install-mongodb'
  entrypoint: 'bash'
  args:
  - '-c'
  - |
    set -e
    set -x
    gcloud compute ssh mongodb-vm --zone ${_ZONE} --command "
    sudo mkdir -p /var/lib/apt/lists/partial
    sudo apt-get clean
    sudo apt-get update -y
    sudo apt-get install -y gnupg curl
    curl -fsSL https://www.mongodb.org/static/pgp/server-4.4.asc | sudo apt-key add -
    echo 'deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu focal/mongodb-org/4.4 multiverse' | sudo tee /etc/apt/sources.list.d/mongodb-org-4.4.list
    sudo apt-get update -y
    sudo apt-get install -y mongodb-org
    sudo systemctl start mongod
    sudo systemctl enable mongod
    sudo systemctl status mongod
    "

# Step 4: Setup MongoDB Auth 
- name: 'gcr.io/cloud-builders/gcloud'
  id: 'setup-mongodb-auth'
  entrypoint: 'bash'
  args:
  - '-c'
  - |
    MONGODB_PASSWORD=$(gcloud secrets versions access latest --secret=mongodb-password)
    gcloud compute ssh mongodb-vm --zone ${_ZONE} --command "
    mongo admin --eval 'db.createUser({user: \"admin\", pwd: \"$${MONGODB_PASSWORD}\", roles: [{role: \"root\", db: \"admin\"}]})' && \
    sudo sed -i 's/#security:/security:\\n  authorization: enabled/' /etc/mongod.conf && \
    sudo systemctl restart mongod
    "
    
# Step 5: Create MongoDB backup service
- name: 'gcr.io/cloud-builders/gcloud'
  id: 'setup-mongodb-backup'
  entrypoint: 'bash'
  args:
  - '-c'
  - |
    # Replace placeholder with actual backup bucket name
    sed -i 's/PLACEHOLDER_BACKUP_BUCKET/${_BACKUP_BUCKET}/g' mongodb_backup.sh
    
    # Create a systemd service file
    cat > mongodb_backup.service << EOL
    [Unit]
    Description=MongoDB Backup Service
    
    [Service]
    Type=oneshot
    ExecStart=/bin/bash -c '/usr/local/bin/mongodb_backup.sh "\$$(cat /etc/mongodb_backup_password)"'
    
    [Install]
    WantedBy=multi-user.target
    EOL
    
    # Copy files to the MongoDB VM
    gcloud compute scp mongodb_backup.sh mongodb_backup.service mongodb-vm:/tmp/ --zone=${_ZONE}
    
    # Set up the backup script and service on the MongoDB VM
    gcloud compute ssh mongodb-vm --zone=${_ZONE} --command="
      sudo mv /tmp/mongodb_backup.sh /usr/local/bin/mongodb_backup.sh && \
      sudo chmod +x /usr/local/bin/mongodb_backup.sh && \
      sudo mv /tmp/mongodb_backup.service /etc/systemd/system/mongodb_backup.service && \
      echo '$$MONGODB_PASSWORD' | sudo tee /etc/mongodb_backup_password > /dev/null && \
      sudo chmod 600 /etc/mongodb_backup_password && \
      sudo systemctl daemon-reload && \
      sudo systemctl enable mongodb_backup.service && \
      echo '0 2 * * * root systemctl start mongodb_backup.service' | sudo tee -a /etc/crontab
    "
  secretEnv: ['MONGODB_PASSWORD']

# Step 6: Mangage GKE Cluster
- name: 'gcr.io/cloud-builders/gcloud'
  id: 'manage-gke-cluster'
  entrypoint: 'bash'
  args:
  - '-c'
  - |
    set -e
    if gcloud container clusters describe ${_CLUSTER_NAME} --zone=${_ZONE} --project=${PROJECT_ID} &>/dev/null; then
      echo "GKE cluster ${_CLUSTER_NAME} exists. Connecting and cleaning up previous deployments..."
      gcloud container clusters get-credentials ${_CLUSTER_NAME} --zone=${_ZONE} --project=${PROJECT_ID}
      
      # Clean up previous deployments
      kubectl delete deployment --all
      kubectl delete service --all
      kubectl delete ingress --all
      kubectl delete configmap --all
      
      # Only delete secrets that are not needed. Be careful with this!
      # kubectl delete secret --all
      
      echo "Previous deployments cleaned up."
    else
      echo "GKE cluster does not exist. Creating..."
      gcloud container clusters create ${_CLUSTER_NAME} \
        --num-nodes=${_NODE_COUNT} \
        --zone=${_ZONE} \
        --project=${PROJECT_ID}
      
      echo "New GKE cluster created."
    fi
    
    # Ensure we have the correct kubectl context
    gcloud container clusters get-credentials ${_CLUSTER_NAME} --zone=${_ZONE} --project=${PROJECT_ID}
    
    echo "Cluster is ready for deployments."

# Step 7: Modify the MongoDB configuration to allow connections from the GKE cluster
- name: 'gcr.io/cloud-builders/gcloud'
  id: 'configure-mongodb'
  entrypoint: 'bash'
  args:
  - '-c'
  - |
    gcloud compute ssh mongodb-vm --zone=${_ZONE} --command="
      sudo sed -i 's/bindIp: 127.0.0.1/bindIp: 0.0.0.0/' /etc/mongod.conf
      sudo systemctl restart mongod
    "
# Step 8: Clone Tasky repository
- name: 'gcr.io/cloud-builders/git'
  id: 'clone-tasky-repo'
  args: ['clone', 'https://github.com/jeffthorne/tasky.git']

# Step 9: Create wizexercise.txt
- name: 'ubuntu'
  id: 'create-wizexercise-file'
  entrypoint: 'bash'
  args:
  - '-c'
  - |
    echo "WIZ.IO IS AWESOME" > wizexercise.txt

# Step 10: Build Docker image
- name: 'gcr.io/cloud-builders/docker'
  id: 'build-image'
  args: ['build', '-t', 'gcr.io/${PROJECT_ID}/tasky-app:v1', './tasky']

# Step 11: Inject wizexercise.txt into the container
- name: 'gcr.io/cloud-builders/docker'
  id: 'inject-wizexercise-file'
  entrypoint: 'bash'
  args:
  - '-c'
  - |
    container_id=$(docker create gcr.io/${PROJECT_ID}/tasky-app:v1)
    docker cp wizexercise.txt $container_id:/wizexercise.txt
    docker commit $container_id gcr.io/${PROJECT_ID}/tasky-app:v1
    docker rm $container_id
    
# Step 12: Push Docker image
- name: 'gcr.io/cloud-builders/docker'
  id: 'push-image'
  args: ['push', 'gcr.io/${PROJECT_ID}/tasky-app:v1']


# Step 13: Verify the image
- name: 'gcr.io/cloud-builders/gcloud'
  id: 'verify-image'
  entrypoint: 'bash'
  args:
  - '-c'
  - |
    gcloud container images list-tags gcr.io/${PROJECT_ID}/tasky-app

# Step 14: Create Kubernetes secrets
- name: 'gcr.io/cloud-builders/kubectl'
  id: 'create-secrets'
  entrypoint: 'bash'
  args:
  - '-c'
  - |
    MONGODB_PASSWORD=$(gcloud secrets versions access latest --secret=mongodb-password)
    MONGODB_URI="mongodb://admin:$${MONGODB_PASSWORD}@$(gcloud compute instances describe mongodb-vm --zone=${_ZONE} --format='get(networkInterfaces[0].networkIP)'):27017/admin"
    SECRET_KEY=$(gcloud secrets versions access latest --secret=jwt-secret-key)
    kubectl create secret generic tasky-secrets \
      --from-literal=MONGODB_URI="$${MONGODB_URI}" \
      --from-literal=SECRET_KEY="$${SECRET_KEY}" \
      --dry-run=client -o yaml | kubectl apply -f -

# Step 15: Networking
- name: 'gcr.io/cloud-builders/gcloud'
  id: 'configure-networking'
  entrypoint: 'bash'
  args:
  - '-c'
  - |
    set -e
    
    # If the networks are different, set up VPC peering
    if [ "$(gcloud container clusters describe ${_CLUSTER_NAME} --zone=${_ZONE} --format='value(network)')" != "$(gcloud compute instances describe mongodb-vm --zone=${_ZONE} --format='value(networkInterfaces[0].network.basename())')" ]; then
      echo "Setting up VPC peering between cluster network and VM network"
      gcloud compute networks peerings create gke-to-mongodb \
        --network=$(gcloud container clusters describe ${_CLUSTER_NAME} --zone=${_ZONE} --format='value(network)') \
        --peer-network=$(gcloud compute instances describe mongodb-vm --zone=${_ZONE} --format='value(networkInterfaces[0].network.basename())') \
        --auto-create-routes
      
      gcloud compute networks peerings create mongodb-to-gke \
        --network=$(gcloud compute instances describe mongodb-vm --zone=${_ZONE} --format='value(networkInterfaces[0].network.basename())') \
        --peer-network=$(gcloud container clusters describe ${_CLUSTER_NAME} --zone=${_ZONE} --format='value(network)') \
        --auto-create-routes
    fi
    
    # Check if firewall rule exists
    if gcloud compute firewall-rules describe allow-gke-to-mongodb &>/dev/null; then
      echo "Firewall rule 'allow-gke-to-mongodb' exists. Updating..."
      gcloud compute firewall-rules update allow-gke-to-mongodb \
        --source-ranges=$(gcloud container clusters describe ${_CLUSTER_NAME} --zone=${_ZONE} --format='value(clusterIpv4Cidr)') \
        --rules=tcp:27017
    else
      echo "Creating firewall rule 'allow-gke-to-mongodb'..."
      gcloud compute firewall-rules create allow-gke-to-mongodb \
        --direction=INGRESS \
        --priority=1000 \
        --network=$(gcloud compute instances describe mongodb-vm --zone=${_ZONE} --format='value(networkInterfaces[0].network.basename())') \
        --action=ALLOW \
        --rules=tcp:27017 \
        --source-ranges=$(gcloud container clusters describe ${_CLUSTER_NAME} --zone=${_ZONE} --format='value(clusterIpv4Cidr)')
    fi

# Step 16: Apply k8s Configs
- name: 'gcr.io/cloud-builders/kubectl'
  id: 'apply-k8s-config'
  entrypoint: 'bash'
  args:
  - '-c'
  - |
    set -e
    echo "Substituting PROJECT_ID in Kubernetes config..."
    sed -i 's|PROJECT_ID_PLACEHOLDER|'"${PROJECT_ID}"'|g' combined-k8s-config.yaml
    
    echo "Applying Kubernetes configuration:"
    kubectl apply -f combined-k8s-config.yaml
    
    echo "Verifying deployments:"
    kubectl get deployments --all-namespaces
    kubectl get services --all-namespaces
    kubectl get ingress --all-namespaces
    
    echo "Checking pod status:"
    kubectl get pods
    kubectl describe pods

# Step 17: Debug Pod Status
- name: 'gcr.io/cloud-builders/kubectl'
  id: 'debug-pod-status'
  entrypoint: 'bash'
  args:
  - '-c'
  - |
    echo "Debugging pod status:"
    kubectl get pods -o wide
    kubectl describe pods -l app=tasky
    echo "Checking node status:"
    kubectl describe nodes
    echo "Checking events:"
    kubectl get events --sort-by=.metadata.creationTimestamp

# Step 18: Verfiy image
- name: 'gcr.io/cloud-builders/gcloud'
  id: 'check-image-availability'
  entrypoint: 'bash'
  args:
  - '-c'
  - |
    echo "Verifying image in Container Registry:"
    gcloud container images describe gcr.io/${PROJECT_ID}/tasky-app:v1

# Step 19: Check Tasky pod logs
- name: 'gcr.io/cloud-builders/kubectl'
  id: 'check-pod-logs'
  entrypoint: 'bash'
  args:
  - '-c'
  - |
    echo "Waiting for pods to be ready..."
    kubectl wait --for=condition=ready pod -l app=tasky --timeout=300s
    echo "Checking Tasky pod logs:"
    kubectl logs -l app=tasky --all-containers=true

images:
- 'gcr.io/${PROJECT_ID}/tasky-app:v1'
    

availableSecrets:
  secretManager:
  - versionName: projects/${PROJECT_ID}/secrets/mongodb-password/versions/latest
    env: 'MONGODB_PASSWORD'

substitutions:
  _ZONE: us-central1-a
  _CLUSTER_NAME: tasky-cluster
  _NODE_COUNT: "3"
  _MACHINE_TYPE: e2-medium
  _BACKUP_BUCKET: ${PROJECT_ID}-mongodb-backups

options:
  logging: CLOUD_LOGGING_ONLY
